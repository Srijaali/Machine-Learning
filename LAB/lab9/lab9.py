# -*- coding: utf-8 -*-
"""lab9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-80oFE6w3ZtSiQOnZ20QVDGfpgyjhQs_
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df = pd.read_csv("heart.csv")
df.head()
df.info()
df.describe()
df.isnull().sum()

plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

X = df.drop("target", axis=1)
y = df["target"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model_l1 = LogisticRegression(
    penalty="l1",
    solver="liblinear",
    max_iter=500
)

model_l1.fit(X_train_scaled, y_train)

train_l1 = accuracy_score(y_train, model_l1.predict(X_train_scaled))
test_l1 = accuracy_score(y_test, model_l1.predict(X_test_scaled))

print("L1 Training Accuracy:", train_l1)
print("L1 Testing Accuracy:", test_l1)

model_l2 = LogisticRegression(
    penalty="l2",
    solver="lbfgs",
    max_iter=500
)

model_l2.fit(X_train_scaled, y_train)

train_l2 = accuracy_score(y_train, model_l2.predict(X_train_scaled))
test_l2 = accuracy_score(y_test, model_l2.predict(X_test_scaled))

print("L2 Training Accuracy:", train_l2)
print("L2 Testing Accuracy:", test_l2)

model_en = LogisticRegression(
    penalty="elasticnet",
    solver="saga",
    l1_ratio=0.5,
    max_iter=1000
)

model_en.fit(X_train_scaled, y_train)

train_en = accuracy_score(y_train, model_en.predict(X_train_scaled))
test_en = accuracy_score(y_test, model_en.predict(X_test_scaled))

print("Elastic Net Training Accuracy:", train_en)
print("Elastic Net Testing Accuracy:", test_en)

results = pd.DataFrame({
    "Penalty": ["L1", "L2", "Elastic Net"],
    "Train Accuracy": [train_l1, train_l2, train_en],
    "Test Accuracy": [test_l1, test_l2, test_en]
})
results

"""While implementing L1 and Elastic Net penalties in Logistic Regression, I encountered solver-related errors.
Specifically:
L1 penalty does not work with the default solver (lbfgs), so an error appears:
“ValueError: Solver lbfgs does not support L1 penalty”
To fix this, I changed the solver to liblinear (or saga).
Elastic Net only works with solver='saga'.
Attempting to use any other solver results in the error:
“ValueError: l1_ratio is only supported for saga solver with elasticnet penalty”
Additionally, Elastic Net requires an extra parameter:
l1_ratio
l1_ratio = 1 → behaves like L1
l1_ratio = 0 → behaves like L2
0 < l1_ratio < 1 → mix of both
Other parameter changes:
Increased max_iter for Elastic Net and L1 because these penalties converge slower.
Scaling was required because Logistic Regression performs better when features are normalize

task2
"""

iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']

results = []

for solver in solvers:
    try:
        model = LogisticRegression(solver=solver, max_iter=2000)
        model.fit(X_train_scaled, y_train)

        train_acc = accuracy_score(y_train, model.predict(X_train_scaled))
        test_acc = accuracy_score(y_test, model.predict(X_test_scaled))

        results.append([solver, train_acc, test_acc])

    except Exception as e:
        results.append([solver, "Error", "Error"])
df_results_iris = pd.DataFrame(
    results,
    columns=["Solver", "Training Accuracy", "Testing Accuracy"]
)
df_results_iris

"""all solvers performed extremely well on the Iris dataset because:
The dataset is small (150 samples)
Features are clean and well-separated
Problem is multi-class but simple
Effect of solvers
Solver	Behavior
lbfgs	Stable, fast, performs best for small/medium multiclass problems.
newton-cg	Similar to lbfgs, high accuracy, strong for multiclass.
newton-cholesky	Very fast convergence, stable like newton-cg.
liblinear	Designed for binary classification; still works but slower and less ideal for multiclass.
sag	Good for large datasets but less stable on very small datasets.
saga	Works with all penalties, handles large datasets; slightly slower here.
Matching sklearn documentation?
Yes — sklearn says:
lbfgs, newton-cg, newton-cholesky → best for multiclass and small/medium datasets
liblinear → best for binary, not ideal for large or multiclass
sag/saga → best for large datasets
This matches our observations: the large-dataset solvers (sag, saga) did not outperform lbfgs on Iris because the dataset is too small.
Best solver for Iris? Why?
lbfgs or newton-cholesky
Fast convergence
Stable
Performs best on small to medium-sized multiclass problems
"""

import pandas as pd

df = pd.read_csv("heart.csv")   # Kaggle heart disease dataset

X = df.drop("target", axis=1)
y = df["target"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']

results_heart = []

for solver in solvers:
    try:
        model = LogisticRegression(solver=solver, max_iter=3000)
        model.fit(X_train_scaled, y_train)

        train_acc = accuracy_score(y_train, model.predict(X_train_scaled))
        test_acc = accuracy_score(y_test, model.predict(X_test_scaled))

        results_heart.append([solver, train_acc, test_acc])

    except Exception as e:
        results_heart.append([solver, "Error", "Error"])
df_results_heart = pd.DataFrame(
    results_heart,
    columns=["Solver", "Training Accuracy", "Testing Accuracy"]
)
df_results_heart

"""es — the Heart Disease dataset has 303 samples, roughly twice the size of Iris (150).
While still small, this dataset begins to show solver differences more clearly.
Observations
lbfgs, newton-cg, newton-cholesky
Stable and high testing accuracy
Converged smoothly
Best overall
liblinear
Works well because this is binary classification,
so performance is better than on Iris.
sag & saga
Show slight instability/noise due to dataset size not being “large” enough
Improve only when dataset becomes very large (10k+ samples)
Which solver is best for Heart Disease? Why?
Same conclusion as Iris:
lbfgs or newton-cholesky work best:
Fast convergence
High accuracy
Good for medium-size datasets
Ideal for binary/multiclass logistic regression
Does solver performance change due to dataset size?
On Iris (small, very clean) → all solvers perform similarly
On Heart Disease (slightly larger, noisier) →
large-dataset solvers (sag / saga) become less stable
while lbfgs & newton-family remain strong.
So yes — solver performance is related to dataset size and structure.
"""

# task 3
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr = LogisticRegression(max_iter=2000)
lr.fit(X_train_scaled, y_train)

lr_train_acc = accuracy_score(y_train, lr.predict(X_train_scaled))
lr_test_acc = accuracy_score(y_test, lr.predict(X_test_scaled))

lr_train_acc, lr_test_acc

per = Perceptron(max_iter=2000, eta0=0.01, random_state=42)
per.fit(X_train_scaled, y_train)

per_train_acc = accuracy_score(y_train, per.predict(X_train_scaled))
per_test_acc = accuracy_score(y_test, per.predict(X_test_scaled))

per_train_acc, per_test_acc

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Perceptron"],
    "Training Accuracy": [lr_train_acc, per_train_acc],
    "Testing Accuracy": [lr_test_acc, per_test_acc]
})

results

"""Model	Behavior
Logistic Regression	Usually achieves higher & more stable accuracy.
Perceptron	Slightly lower accuracy and more variance due to being a harder-line classifier.
Logistic Regression tends to:
Converge better
Generalize more smoothly
Produce probabilistic outputs
The Perceptron, being a linear classifier using a hard threshold, is very sensitive to:
Learning rate
Feature scaling
Data not being perfectly linearly separable
Thus, Logistic Regression almost always performs more reliably on the Iris dataset.
"""

# task4
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
df = pd.read_csv("fraud.csv")   # change filename if needed
df.head()
df.info()
df.describe()
df.isnull().sum()
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), cmap="coolwarm")
plt.show()

df = df.dropna()

X = df.drop("is_fraud", axis=1)
y = df["is_fraud"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(64, activation='tanh'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')   # binary classification
])
model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)
history = model.fit(
    X_train_scaled,
    y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

plt.figure(figsize=(14,5))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.legend()
plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.title("Loss")

plt.show()
y_pred = model.predict(X_test_scaled)
y_pred = (y_pred > 0.5).astype(int)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Test Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1 Score:", f1)

print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.show()